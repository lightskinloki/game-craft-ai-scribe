
import { useState, useEffect, useCallback } from 'react';
import { pipeline, Pipeline } from '@huggingface/transformers';

interface LocalAIState {
  isLoading: boolean;
  isAvailable: boolean;
  modelName: string | null;
  error: string | null;
}

export const useLocalAI = () => {
  const [state, setState] = useState<LocalAIState>({
    isLoading: false,
    isAvailable: false,
    modelName: null,
    error: null,
  });
  
  const [pipeline_, setPipeline] = useState<Pipeline | null>(null);

  // Scan for GGUF files and initialize model
  useEffect(() => {
    const initializeModel = async () => {
      setState(prev => ({ ...prev, isLoading: true, error: null }));
      
      try {
        // Try to find and load a GGUF model from /public/models/
        const commonModelNames = [
          'model.gguf',
          'chat-model.gguf',
          'llama.gguf',
          'phi.gguf',
          'gemma.gguf'
        ];
        
        let modelLoaded = false;
        let loadedModelName = '';
        
        for (const modelName of commonModelNames) {
          try {
            const modelPath = `/models/${modelName}`;
            console.log(`Attempting to load local model: ${modelPath}`);
            
            // Try to load the model
            const textGenerator = await pipeline('text-generation', modelPath, {
              device: 'webgpu', // Try WebGPU first, will fallback to CPU
            });
            
            setPipeline(textGenerator);
            loadedModelName = modelName;
            modelLoaded = true;
            console.log(`Successfully loaded local model: ${modelName}`);
            break;
          } catch (error) {
            console.log(`Model ${modelName} not found or failed to load:`, error);
            continue;
          }
        }
        
        setState(prev => ({
          ...prev,
          isLoading: false,
          isAvailable: modelLoaded,
          modelName: loadedModelName || null,
          error: modelLoaded ? null : 'No local GGUF models found in /public/models/',
        }));
        
      } catch (error) {
        console.error('Error initializing local AI:', error);
        setState(prev => ({
          ...prev,
          isLoading: false,
          isAvailable: false,
          error: error instanceof Error ? error.message : 'Failed to initialize local AI',
        }));
      }
    };

    initializeModel();
  }, []);

  const generateText = useCallback(async (prompt: string): Promise<string> => {
    if (!pipeline_ || !state.isAvailable) {
      throw new Error('Local AI model not available');
    }

    try {
      console.log('Generating text with local model...');
      
      const result = await pipeline_(prompt, {
        max_new_tokens: 500,
        temperature: 0.7,
        do_sample: true,
      });
      
      // Extract generated text (format may vary by model)
      const generatedText = Array.isArray(result) ? result[0]?.generated_text : result?.generated_text;
      
      if (!generatedText) {
        throw new Error('No text generated by local model');
      }
      
      // Remove the original prompt from the response if it's included
      const responseText = generatedText.replace(prompt, '').trim();
      return responseText || generatedText;
      
    } catch (error) {
      console.error('Local AI generation error:', error);
      throw error;
    }
  }, [pipeline_, state.isAvailable]);

  return {
    ...state,
    generateText,
  };
};
