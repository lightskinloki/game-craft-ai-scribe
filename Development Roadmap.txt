
GameCraft AI Scribe - Development Roadmap (v11)
Goal: Transform the functional prototype of GameCraft AI Scribe into a robust, user-friendly, AI-assisted game development environment, optimized for Phaser 3 users, with comprehensive developer aids and learning tools. The app will be distributed as a one-time payment tool, prioritizing accessibility for $500 PCs (8GB RAM, no GPU) while supporting any GGUF or SafeTensors model, from 1.5B parameters to hypothetical 300B+ models like Grok 3 for high-end users.

Why Local Inference?
GameCraft AI Scribe initially relied on the Gemini API for its AI assistant, leveraging its free plan to align with our vision of an affordable, one-time payment tool. However, Gemini's decision to discontinue its free plan introduced unsustainable recurring costs, conflicting with our goal of accessibility and offline usability. To address this, we've shifted to local inference using GGUF and SafeTensors models, offering:
Offline Functionality: No internet required after setup, ensuring accessibility anywhere.
No Recurring Costs: Eliminates API fees, maintaining the one-time payment model.
Hardware Inclusivity: Supports $500 PCs with 1.5B models while allowing high-end users to run larger models (e.g., 7B, 13B, or Grok 3).
Model Flexibility: A model-agnostic design lets users choose any GGUF or SafeTensors model, ensuring compatibility and future-proofing.
This shift empowers developers with a private, cost-effective, and versatile tool that scales with their hardware and needs.

Development Phases

✅ Phase 1: Core Enhancements & Stability (Complete)
Focus: Finalize core functionality, ensure stability, and integrate a model-agnostic local inference system supporting 1.5B models on $500 PCs (8GB RAM, no GPU) while allowing any GGUF/SafeTensors model.
Status: ✅ Complete - All core features implemented and tested.

Key Features Implemented:
✅ Project Saving & Loading (Local)
✅ Enhanced Multi-File Support (Multiple .js + index.html) w/ File Explorer (in tab)
✅ Phaser Console Log Access
✅ Manual Game Preview Update (via "Run" Button)
✅ Basic Project Export (Self-contained w/ assets)
✅ Layout: Vertical Middle Panel (Resizable), Tabbed Right Panel (Files/Editor)
✅ UI Polish & Bug Fixing / Testing - Core features thoroughly tested
✅ Model-Agnostic Inference Integration:
  ✅ Built modular backend using useLocalAI hook with unified API
  ✅ Support for model loading via plug-and-play interface (local file upload)
  ✅ Optimized for 1.5B+ models (e.g., TinyLlama-1.1B-GGUF, Qwen-1.8B-SafeTensors)
  ✅ Created setup wizard with hardware detection and model recommendations
  ✅ Added model upload interface with format validation (GGUF/SafeTensors)
  ✅ Implemented local model storage (IndexedDB) for offline functionality
  ✅ Added lite mode fallback for systems without models
✅ Packaging Prep:
  ✅ Prototype build ready (web-based with local inference)
  ✅ Fully offline functionality implemented post-setup
  ✅ Successfully tested on various hardware configurations

Deliverables: ✅ Complete
✅ Stable application with polished UI and core features
✅ Model-agnostic inference system supporting 1.5B+ models on $500 PCs
✅ Web-based build with setup wizard and hardware detection

🔄 Phase 2: AI Precision, Learning Aids & Workflow (In Progress - 60% Complete)
Focus: Enhance AI functionality for low-end hardware, add learning tools, and improve developer productivity while maintaining model flexibility.

Prioritized Features:
🔄 "Explain Like I'm 5" (ELI5) Mode:
  ✅ Basic AI response system implemented via useLocalAI
  🔄 Need to add specific ELI5 prompt templates for simple explanations
  🔄 Cache responses locally to minimize inference load
🔄 AI Line Number Awareness:
  ✅ Code context parsing available in codeChangeParser utility
  🔄 Need to implement line-specific suggestions and error highlighting
  🔄 Test thoroughly with 1.5B models for Phaser-specific accuracy
⭐ Project Templates (incl. Minecraft Clone Tutorial):
  🔄 Template storage system needed (JSON/ZIP format)
  🔄 AI-generated comments integration
  🔄 Optimize for minimal resource use (small asset sizes)

Editor Enhancements:
🔄 Integrate offline linters (ESLint) and formatters (Prettier)
🔄 Use AI to suggest fixes based on linting output, optimized for 1.5B models

🔄 Code Snippet Library (Local Storage):
  🔄 Store snippets in LocalStorage/SQLite
  🔄 Enable AI to suggest snippets based on context

🔄 Integrated Phaser Documentation Search:
  🔄 Bundle compressed Phaser docs (JSON) for offline search
  🔄 Use AI to summarize or contextualize results with low compute

✅ Asset Manager Improvements:
  ✅ Basic asset management implemented in AssetManager component
  ✅ File upload and organization features
  🔄 Generate thumbnails locally (Canvas API) - partially implemented
  🔄 Pair assets with code via lightweight AI analysis

✅ Persistent Settings:
  ✅ Save model choices and UI preferences in LocalStorage via useLocalAI

🔄 Basic Performance Monitoring:
  ✅ Console output integration for debugging
  🔄 Add FPS counter using Phaser tools
  🔄 Use AI to suggest lightweight optimizations (e.g., sprite batching)

Technical Tasks:
✅ Model loading and inference system (useLocalAI hook)
✅ Support model hot-swapping via ModelUpload component
🔄 Test AI accuracy for Phaser tasks (e.g., scene transitions, physics) on 1.5B models
✅ Implement hardware fallback to lite mode via HardwareDetection component

Deliverables (60% Complete):
✅ Basic AI assistant with model flexibility and hardware detection
🔄 Offline workflow with templates, docs, and snippets (30% complete)
✅ Optimized performance on $500 PCs

Phase 3: Advanced AI, Polish & Sharing Foundations
Focus: Refine AI interactions, enhance editor tools, and enable offline project/template sharing.

Key Features:
⭐ AI Conversation History:
  Store history in SQLite for low-memory context retrieval
  Support lightweight context for 1.5B models
⭐ Advanced Editor:
  Add search/replace and autocomplete using 1.5B models with minimal compute
  Suggest real-time code completions (e.g., method suggestions)
⭐ Contextual AI Prompts/Refactoring:
  Optimize prompts for refactoring Phaser code (e.g., optimize loops)
  Test on complex game code with 1.5B models
⭐ AI Tutorials:
  Generate interactive tutorials (JSON/Markdown) adapted to user skill levels
  Store locally for offline use
⭐ Enhanced Onboarding:
  Guided setup with hardware detection and model recommendations (1.5B default)
⭐ Local Project Sharing:
  Export projects as ZIPs with assets and code
  Support drag-and-drop import for shared projects
⭐ Template Sharing:
  Save/export templates locally as ZIPs
  Support import via file picker

Technical Tasks:
Optimize memory for larger projects on 8GB systems
Add optional GPU support (CUDA/Metal) for mid/high-end users
Test high-end models (e.g., 13B, hypothetical Grok 3) on 32GB+ systems
Finalize packaging with bundled 1.5B model and inference engine

Deliverables:
Polished app with advanced AI and editor features
Fully offline sharing and template system
Flexible model support (1.5B to 300B+)

Phase 4: Distribution & Post-Launch
Focus: Package the app for one-time purchase and ensure easy installation on $500 PCs.

Key Tasks:
Application Packaging:
  Finalize build with bundled inference engine and a 1.5B model (e.g., TinyLlama-GGUF)
  Support Windows, macOS, Linux with single-click installers
Dependency Bundling:
  Include inference engine and minimal dependencies
  Offer model size options (1.5B default, 7B+ optional) during setup
Configuration Management:
  Add UI for model path input and hardware optimization
  Save configs locally
Auto-Updates:
  Implement optional local update checker (USB/offline support)
Desktop Integration:
  Create shortcuts/icons for one-click launch
Distribution:
  Prepare for one-time purchase via platforms like Gumroad or itch.io
  Provide clear hardware requirements (8GB RAM, 256GB SSD) and setup guides

Deliverables:
Fully offline, packaged app ready for sale
Seamless installation for $500 PCs

Phase 5: Future-Proofing & Scalability
Focus: Ensure long-term relevance through modularity, community contributions, and support for emerging technologies while maintaining accessibility.

Key Features:
⭐ Modular Plugin System:
  Develop a JSON-based plugin API for new features (e.g., Godot/Unity support, AI tools)
  Support community plugins for templates and prompts, stored locally
⭐ WebAssembly Integration:
  Port inference to WebAssembly for browser-based use on Chromebooks/low-end devices
  Optimize for 1.5B models using WASI-compatible runtimes (e.g., Wasmtime)
⭐ AI Model Evolution:
  Add a compatibility layer for future model formats (e.g., ONNX, potential xAI formats)
  Support mixture-of-experts (MoE) models (e.g., Mixtral) for efficient inference
  Enable one-time model/prompt downloads via USB or internet
Community Contribution Framework:
  Create a local repository (SQLite) for offline template/plugin sharing via USB/LAN
  Provide a contribution guide for developers
Hardware Scalability:
  Implement dynamic resource allocation (model size, context window) based on hardware
  Support distributed inference for high-end models (e.g., Grok 3 on 128GB+ systems)
  Optimize for emerging budget hardware (e.g., ARM, RISC-V)
AI Training Integration:
  Allow local fine-tuning (e.g., LoRA) for Phaser tasks on 16GB+ systems
  Provide pre-tuned prompt sets for 1.5B models
Accessibility Enhancements:
  Add low-resource mode for 4GB RAM systems using micro-models (<1B)
  Support screen reader and keyboard navigation
Future Tech Readiness:
  Prepare for AI hardware accelerators (e.g., Intel NPUs) and next-gen OSes (e.g., Windows 12)

Technical Tasks:
Develop plugin API with sample plugins (e.g., Godot adapter)
Optimize WebAssembly for 1.5B models in Chrome/Firefox
Create model adapter for future formats and test MoE models
Build local repository for community contributions
Test scalability on $500 PCs and high-end systems
Add LoRA fine-tuning scripts and accessibility features

Deliverables:
Plugin system for community and engine extensions
WebAssembly build for browser-based use
Flexible model support for future formats and high-end models
Local repository for offline community contributions
Scalable performance from 4GB to 128GB+ systems

Future Considerations / Phase X: Advanced, Experimental & Specialized Features
Focus: Explore cutting-edge integrations and community features while maintaining accessibility.

Key Features:
Full AI Game Creation:
  Plan, code, and test prototypes with 1.5B models, scalable to MoE or Grok 3
AI-Assisted Tilemap Editor:
  Use tiny vision models (<2GB) for low-end systems, larger for high-end
Version Control:
  Local Git integration with AI-assisted commit messages
Build/Deployment Integration (Web):
  Support lightweight web builds for Phaser games
Collaboration:
  Local collaboration tools (e.g., LAN-based sharing)
State Machine Visualization:
  AI-driven visual tools for game logic
Accessibility Checker:
  Ensure games meet accessibility standards
Visual Editor Elements:
  Lightweight drag-and-drop with AI code generation
Integrated Debugger:
  AI-driven error fixes optimized for 1.5B models
AI-Driven Testing:
  Automate playtesting with local AI
Mobile Support:
  Extend to Android/iOS tablets with lightweight models
VR/AR Integration:
  Support basic VR/AR game dev (e.g., Three.js)

Recommended Models
The app supports any GGUF or SafeTensors model for flexibility. Recommended models include:

Low-End ($500 PC, 8GB RAM):
TinyLlama-1.1B-GGUF (4-bit, ~2GB): Fast, lightweight, ideal for basic code tasks
Qwen-1.8B-SafeTensors (4-bit, ~2.5GB): Strong for code and explanations

Mid-Range (16GB RAM, Optional GPU):
CodeLlama-7B-GGUF (4-bit, ~4-6GB): Excellent for Phaser-specific tasks
Mistral-7B-SafeTensors (4-bit, ~5GB): Versatile for code and learning aids

High-End (32GB+ RAM, GPU):
Llama-13B-GGUF (4-bit, ~8-10GB): For advanced users
Hypothetical Grok 3: Manual path input for 128GB+ RAM, high-end GPU systems

Models can be sourced from Hugging Face or a project-hosted server, with checksums for integrity.

Implementation Notes
✅ Modular Backend: Unified API for GGUF and SafeTensors via useLocalAI hook
✅ Optimization: Model upload system with format validation and local storage
✅ User Experience: Setup wizard with hardware detection, model upload interface
✅ Security: SafeTensors support for secure model loading; no telemetry or external data sharing
✅ Testing: Validated on various hardware configurations and browser environments

Next Steps
Immediate (Phase 2 - Current Focus):
🔄 Complete AI line number awareness and ELI5 mode implementation
🔄 Add project templates with AI-generated comments
🔄 Implement code snippet library and Phaser documentation search

Short-Term:
Build advanced editor features (autocomplete, refactoring suggestions)
Add AI conversation history and contextual prompts
Implement local project/template sharing system

Long-Term:
Develop plugin system and WebAssembly support in Phase 5 for future-proofing
Maintain accessibility, offline-first design, and one-time payment model
Explore advanced features like AI-driven testing and visual editors

Contributing
We welcome contributions from the community! Check the CONTRIBUTING.md file for guidelines on submitting plugins, templates, or code improvements. Feedback and bug reports can be filed via GitHub Issues.
