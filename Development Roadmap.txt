GameCraft AI Scribe - Development Roadmap (v10)
Goal: Transform the functional prototype of GameCraft AI Scribe into a robust, user-friendly, AI-assisted game development environment, optimized for Phaser 3 users, with comprehensive developer aids and learning tools. The app will be distributed as a one-time payment tool, prioritizing accessibility for $500 PCs (8GB RAM, no GPU) while supporting any GGUF or SafeTensors model, from 1.5B parameters to hypothetical 300B+ models like Grok 3 for high-end users.
Why Local Inference?
GameCraft AI Scribe initially relied on the Gemini API for its AI assistant, leveraging its free plan to align with our vision of an affordable, one-time payment tool. However, Gemini’s decision to discontinue its free plan introduced unsustainable recurring costs, conflicting with our goal of accessibility and offline usability. To address this, we’ve shifted to local inference using GGUF and SafeTensors models, offering:
Offline Functionality: No internet required after setup, ensuring accessibility anywhere.
No Recurring Costs: Eliminates API fees, maintaining the one-time payment model.
Hardware Inclusivity: Supports $500 PCs with 1.5B models while allowing high-end users to run larger models (e.g., 7B, 13B, or Grok 3).
Model Flexibility: A model-agnostic design lets users choose any GGUF or SafeTensors model, ensuring compatibility and future-proofing.
This shift empowers developers with a private, cost-effective, and versatile tool that scales with their hardware and needs.
Development Phases
✅ Phase 1: Core Enhancements & Stability (Functionally Complete - Pending Polish)
Focus: Finalize core functionality, ensure stability, and integrate a model-agnostic local inference system supporting 1.5B models on $500 PCs (8GB RAM, no GPU) while allowing any GGUF/SafeTensors model.
Status: All core features are implemented. Final polish, testing, and inference integration are in progress.
Key Features Implemented:
✅ Project Saving & Loading (Local)
✅ Enhanced Multi-File Support (Multiple .js + index.html) w/ File Explorer (in tab)
✅ Phaser Console Log Access
✅ Manual Game Preview Update (via "Run" Button)
✅ Basic Project Export (Self-contained w/ assets)
✅ Layout: Vertical Middle Panel (Resizable), Tabbed Right Panel (Files/Editor)
Remaining Tasks for Phase 1:
🔄 UI Polish & Bug Fixing / Testing: IMMEDIATE & ONLY FOCUS for Phase 1
Thoroughly test ALL Phase 1 features: Save/Load, Export, Console Log, Preview ("Run" Button), File Management, Mode Switching.
Evaluate layout usability: Test resizing panels, switching tabs, and content display/responsiveness.
Fix glitches in functionality or display.
Improve loading indicators and user feedback consistency.
General code cleanup and consistency checks.
🔄 Model-Agnostic Inference Integration:
Build a modular backend using Llama.cpp (GGUF) and Hugging Face Transformers (SafeTensors) with a unified API.
Support model loading via a plug-and-play interface (local file path or one-time URL download).
Optimize for 1.5B models (e.g., TinyLlama-1.1B-GGUF, Qwen-1.8B-SafeTensors) in 4-bit quantization (~2-3GB RAM).
Create a setup wizard to:
Detect hardware (CPU, RAM, GPU).
Recommend models (1.5B for 8GB, 7B for 16GB+, manual selection for high-end models like Grok 3).
Allow one-time model downloads from Hugging Face or project-hosted server.
Cache AI responses locally (SQLite/JSON) to reduce inference load.
Add a lite mode (rule-based suggestions) for <8GB RAM systems.
Implement batch processing and swap memory support for 7B models on 8GB systems.
🔄 Packaging Prep:
Prototype a Tauri build (lightweight alternative to Electron) with bundled Llama.cpp and a 1.5B model.
Ensure fully offline functionality post-setup.
Test on $500 PCs (e.g., Lenovo IdeaPad, 8GB RAM, Ryzen 3).
Deliverables:
Stable application with polished UI and core features.
Model-agnostic inference system supporting 1.5B models on $500 PCs.
Initial Tauri build with setup wizard and lite mode.
Phase 2: AI Precision, Learning Aids & Workflow
Focus: Enhance AI functionality for low-end hardware, add learning tools, and improve developer productivity while maintaining model flexibility.
Prioritized Features:
⭐ "Explain Like I'm 5" (ELI5) Mode:
Use lightweight prompts for 1.5B models to generate simple code explanations.
Cache responses locally to minimize inference load.
⭐ AI Line Number Awareness:
Parse code context (line numbers, file structure) for accurate suggestions.
Test thoroughly with 1.5B models for Phaser-specific accuracy.
⭐ Project Templates (incl. Minecraft Clone Tutorial):
Store templates locally (JSON/ZIP) with AI-generated comments.
Optimize for minimal resource use (small asset sizes).
Editor Enhancements:
Integrate offline linters (ESLint) and formatters (Prettier).
Use AI to suggest fixes based on linting output, optimized for 1.5B models.
Code Snippet Library (Local Storage):
Store snippets in LocalStorage/SQLite.
Enable AI to suggest snippets based on context.
Integrated Phaser Documentation Search:
Bundle compressed Phaser docs (JSON) for offline search.
Use AI to summarize or contextualize results with low compute.
Asset Manager Improvements:
Generate thumbnails locally (Canvas API).
Pair assets with code via lightweight AI analysis.
Persistent Settings:
Save model choices and UI preferences in LocalStorage.
Basic Performance Monitoring:
Add FPS counter using Phaser tools.
Use AI to suggest lightweight optimizations (e.g., sprite batching).
Technical Tasks:
Optimize inference for 1.5B models (e.g., 512-token context window).
Support model hot-swapping without app restart.
Test AI accuracy for Phaser tasks (e.g., scene transitions, physics) on 1.5B models.
Implement hardware fallback to lite mode for failed inference.
**Deliverablesmettre
ables**:
Accessible AI assistant with precise code assistance and learning aids.
Fully offline workflow with templates, docs, and snippets.
Optimized performance on $500 PCs.
Phase 3: Advanced AI, Polish & Sharing Foundations
Focus: Refine AI interactions, enhance editor tools, and enable offline project/template sharing.
Key Features:
AI Conversation History:
Store history in SQLite for low-memory context retrieval.
Support lightweight context for 1.5B models.
Advanced Editor:
Add search/replace and autocomplete using 1.5B models with minimal compute.
Suggest real-time code completions (e.g., method suggestions).
Contextual AI Prompts/Refactoring:
Optimize prompts for refactoring Phaser code (e.g., optimize loops).
Test on complex game code with 1.5B models.
AI Tutorials:
Generate interactive tutorials (JSON/Markdown) adapted to user skill levels.
Store locally for offline use.
Enhanced Onboarding:
Guided setup with hardware detection and model recommendations (1.5B default).
Local Project Sharing:
Export projects as ZIPs with assets and code.
Support drag-and-drop import for shared projects.
Template Sharing:
Save/export templates locally as ZIPs.
Support import via file picker.
Technical Tasks:
Optimize memory for larger projects on 8GB systems.
Add optional GPU support (CUDA/Metal) for mid/high-end users.
Test high-end models (e.g., 13B, hypothetical Grok 3) on 32GB+ systems.
Finalize Tauri packaging with bundled 1.5B model and inference engine.
Deliverables:
Polished app with advanced AI and editor features.
Fully offline sharing and template system.
Flexible model support (1.5B to 300B+).
Phase 4: Distribution & Post-Launch
Focus: Package the app for one-time purchase and ensure easy installation on $500 PCs.
Key Tasks:
Application Packaging:
Finalize Tauri build with bundled Llama.cpp and a 1.5B model (e.g., TinyLlama-GGUF).
Support Windows, macOS, Linux with single-click installers.
Dependency Bundling:
Include Llama.cpp binary and minimal Python for Transformers.
Offer model size options (1.5B default, 7B+ optional) during setup.
Configuration Management:
Add UI for model path input and hardware optimization.
Save configs locally.
Auto-Updates:
Implement optional local update checker (USB/offline support).
Desktop Integration:
Create shortcuts/icons for one-click launch.
Distribution:
Prepare for one-time purchase via platforms like Gumroad or itch.io.
Provide clear hardware requirements (8GB RAM, 256GB SSD) and setup guides.
Deliverables:
Fully offline, packaged app ready for sale.
Seamless installation for $500 PCs.
Phase 5: Future-Proofing & Scalability
Focus: Ensure long-term relevance through modularity, community contributions, and support for emerging technologies while maintaining accessibility.
Key Features:
⭐ Modular Plugin System:
Develop a JSON-based plugin API for new features (e.g., Godot/Unity support, AI tools).
Support community plugins for templates and prompts, stored locally.
⭐ WebAssembly Integration:
Port inference to WebAssembly for browser-based use on Chromebooks/low-end devices.
Optimize for 1.5B models using WASI-compatible runtimes (e.g., Wasmtime).
⭐ AI Model Evolution:
Add a compatibility layer for future model formats (e.g., ONNX, potential xAI formats).
Support mixture-of-experts (MoE) models (e.g., Mixtral) for efficient inference.
Enable one-time model/prompt downloads via USB or internet.
Community Contribution Framework:
Create a local repository (SQLite) for offline template/plugin sharing via USB/LAN.
Provide a contribution guide for developers.
Hardware Scalability:
Implement dynamic resource allocation (model size, context window) based on hardware.
Support distributed inference for high-end models (e.g., Grok 3 on 128GB+ systems).
Optimize for emerging budget hardware (e.g., ARM, RISC-V).
AI Training Integration:
Allow local fine-tuning (e.g., LoRA) for Phaser tasks on 16GB+ systems.
Provide pre-tuned prompt sets for 1.5B models.
Accessibility Enhancements:
Add low-resource mode for 4GB RAM systems using micro-models (<1B).
Support screen reader and keyboard navigation.
Future Tech Readiness:
Prepare for AI hardware accelerators (e.g., Intel NPUs) and next-gen OSes (e.g., Windows 12).
Technical Tasks:
Develop plugin API with sample plugins (e.g., Godot adapter).
Optimize WebAssembly for 1.5B models in Chrome/Firefox.
Create model adapter for future formats and test MoE models.
Build local repository for community contributions.
Test scalability on $500 PCs and high-end systems.
Add LoRA fine-tuning scripts and accessibility features.
Deliverables:
Plugin system for community and engine extensions.
WebAssembly build for browser-based use.
Flexible model support for future formats and high-end models.
Local repository for offline community contributions.
Scalable performance from 4GB to 128GB+ systems.
Future Considerations / Phase X: Advanced, Experimental & Specialized Features
Focus: Explore cutting-edge integrations and community features while maintaining accessibility.
Key Features:
Full AI Game Creation:
Plan, code, and test prototypes with 1.5B models, scalable to MoE or Grok 3.
AI-Assisted Tilemap Editor:
Use tiny vision models (<2GB) for low-end systems, larger for high-end.
Version Control:
Local Git integration with AI-assisted commit messages.
Build/Deployment Integration (Web):
Support lightweight web builds for Phaser games.
Collaboration:
Local collaboration tools (e.g., LAN-based sharing).
State Machine Visualization:
AI-driven visual tools for game logic.
Accessibility Checker:
Ensure games meet accessibility standards.
Visual Editor Elements:
Lightweight drag-and-drop with AI code generation.
Integrated Debugger:
AI-driven error fixes optimized for 1.5B models.
AI-Driven Testing:
Automate playtesting with local AI.
Mobile Support:
Extend to Android/iOS tablets with lightweight models.
VR/AR Integration:
Support basic VR/AR game dev (e.g., Three.js).
Recommended Models
The app supports any GGUF or SafeTensors model for flexibility. Recommended models include:
Low-End ($500 PC, 8GB RAM):
TinyLlama-1.1B-GGUF (4-bit, ~2GB): Fast, lightweight, ideal for basic code tasks.
Qwen-1.8B-SafeTensors (4-bit, ~2.5GB): Strong for code and explanations.
Mid-Range (16GB RAM, Optional GPU):
CodeLlama-7B-GGUF (4-bit, ~4-6GB): Excellent for Phaser-specific tasks.
Mistral-7B-SafeTensors (4-bit, ~5GB): Versatile for code and learning aids.
High-End (32GB+ RAM, GPU):
Llama-13B-GGUF (4-bit, ~8-10GB): For advanced users.
Hypothetical Grok 3: Manual path input for 128GB+ RAM, high-end GPU systems.
Models can be sourced from Hugging Face or a project-hosted server, with checksums for integrity.
Implementation Notes
Modular Backend: Unified API for GGUF (Llama.cpp) and SafeTensors (Transformers), supporting custom model paths.
Optimization: 4-bit quantization, small context windows (512 tokens), and memory-mapped models for 8GB systems.
User Experience: Setup wizard with hardware detection, inference progress indicators, and a model compatibility guide.
Security: SafeTensors for secure model loading; no telemetry or external data sharing.
Testing: Validate on $500 PCs (e.g., Lenovo IdeaPad, 8GB RAM) and high-end systems (32GB+). Ensure Phaser-specific AI accuracy.
Next Steps
Immediate (Phase 1):
Complete UI polish and bug fixing.
Prototype inference with TinyLlama-1.1B-GGUF on a $500 PC.
Short-Term:
Build setup wizard with hardware detection and model recommendations.
Optimize for 1.5B models (e.g., reduce context window, enable swap).
Long-Term:
Develop plugin system and WebAssembly support in Phase 5 for future-proofing.
Maintain accessibility, offline-first design, and one-time payment model.
Contributing
We welcome contributions from the community! Check the CONTRIBUTING.md file for guidelines on submitting plugins, templates, or code improvements. Feedback and bug reports can be filed via GitHub Issues.
